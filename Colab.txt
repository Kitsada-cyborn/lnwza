‡∏¢‡∏≠‡∏î ‚Äî ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ú‡∏°‡∏£‡∏ß‡∏° **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏ö** (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Colab ‚Üí ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏î‡πâ‡∏ß‡∏¢ GNN (‡∏´‡∏£‡∏∑‡∏≠ fallback PCA) ‚Üí ‡πÄ‡∏≠‡∏≤ embedding ‡∏ú‡∏™‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‚Üí ‡πÄ‡∏ó‡∏£‡∏ô LightGBM ‚Üí ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î‡πÇ‡∏Ñ‡πâ‡∏î **‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå** (copy-paste ‡πÄ‡∏Ç‡πâ‡∏≤ Colab ‡∏ó‡∏µ‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ú‡∏°‡πÅ‡∏Å‡πâ/‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏ô environment ‡∏ô‡∏µ‡πâ

> **‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ú‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á/‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ (‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ)**
>
> * [‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î `cleaned_combined.csv`](sandbox:/mnt/data/cleaned_combined.csv) ‚Äî ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏° (normalized) ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
> * [‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î `mapping_decisions.json`](sandbox:/mnt/data/mapping_decisions.json) ‚Äî ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏°‡∏õ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (traceability)

---

## ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°

1. ‡πÄ‡∏õ‡∏¥‡∏î Colab ‚Üí ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ç‡πâ‡∏≤ Colab)
2. ‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á (‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ)
3. ‡∏ñ‡πâ‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyTorch+PyG ‡∏¢‡∏≤‡∏Å ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ **fallback (PCA)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡∏£‡∏ô LightGBM ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‚Äî ‡∏ú‡∏°‡πÉ‡∏´‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ß‡πâ

---

# Colab-ready cells ‚Äî ‡∏Ñ‡∏±‡∏î-‡∏ß‡∏≤‡∏á‡∏ó‡∏µ‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 0 ‚Äî ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 0 ‚Äî ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á libs (‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
!pip install pandas numpy scikit-learn lightgbm networkx rapidfuzz matplotlib shap --quiet

# PyTorch + PyG: (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô GNN ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏ô‡∏µ‡πâ‡∏ï‡∏≤‡∏°‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á Colab)
# NOTE: ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á torch + torch_geometric ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö CUDA/CPU ‡∏Ç‡∏≠‡∏á instance ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ fallback PCA ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
# CPU example (‡∏≠‡∏≤‡∏à‡∏ä‡πâ‡∏≤) - ‡∏ñ‡πâ‡∏≤‡∏ï‡∏¥‡∏î error ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyG ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ PCA fallback:
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q
!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cpu.html -q
!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cpu.html -q
!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.13.1+cpu.html -q
!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.13.1+cpu.html -q
!pip install torch-geometric -q
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏û‡πá‡∏Ñ‡πÄ‡∏Å‡∏à‡∏´‡∏•‡∏±‡∏Å (pandas/numpy/LightGBM/etc.) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GNN (PyTorch + PyG) ‚Äî ‡∏ñ‡πâ‡∏≤‡∏ï‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô/‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏° PyG ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ PCA fallback (‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 1 ‚Äî ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ú‡∏°‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏´‡πâ‡πÅ‡∏•‡πâ‡∏ß)

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 1 ‚Äî ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå 2 ‡πÑ‡∏ü‡∏•‡πå (‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå cleaned_combined.csv ‡∏ó‡∏µ‡πà‡∏ú‡∏°‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ô‡∏µ‡πâ)
from google.colab import files
uploaded = files.upload()  # ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå: blacklistaccount.csv, Dataset Mule.csv
# ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå cleaned_combined.csv ‡∏ó‡∏µ‡πà‡∏ú‡∏°‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏ó‡∏ô
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡πÉ‡∏ô Colab ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏õ‡∏∏‡πà‡∏°‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå 2 ‡∏ä‡∏∏‡∏î (‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î `cleaned_combined.csv` ‡∏ó‡∏µ‡πà‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‚Äî ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏° ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏Å‡∏≤‡∏£ clean ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏õ‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î cleaned\_combined.csv)

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 2 ‚Äî (‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å raw) ‡∏£‡∏±‡∏ô Data cleaning & unify (‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤+‡πÉ‡∏´‡∏°‡πà ‚Üí cleaned\_combined.csv)

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 2 ‚Äî ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î (‡∏£‡∏±‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î raw files ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)
import pandas as pd, numpy as np, re, json, os
from datetime import datetime

# ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô Colab (‡πÅ‡∏Å‡πâ‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡∏ñ‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á)
old_path = "blacklistaccount.csv"   # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏° ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î cleaned_combined.csv ‡πÅ‡∏ó‡∏ô
new_path = "Dataset Mule.csv"

def try_read(path):
    encs = ['utf-8','utf-8-sig','cp874','latin1']
    for e in encs:
        try:
            return pd.read_csv(path, encoding=e, low_memory=False)
        except Exception:
            continue
    raise ValueError(f"Cannot read {path}")

df_old = try_read(old_path)
df_new = try_read(new_path)

def find_col(cols, keywords):
    cols_l = [c.lower() for c in cols]
    for kw in keywords:
        for i,c in enumerate(cols_l):
            if kw in c:
                return cols[i]
    return None

def standardize(df, filename):
    cols = list(df.columns)
    mapped = {}
    mapped['date_str'] = find_col(cols, ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà','date','date '])
    mapped['time_str'] = find_col(cols, ['‡πÄ‡∏ß‡∏•‡∏≤','time'])
    mapped['amount'] = find_col(cols, ['‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤','price','amount','‡∏¢‡∏≠‡∏î','value'])
    mapped['from_bank'] = find_col(cols, ['source bank','souce bank','from bank','‡∏ä‡∏∑‡πà‡∏≠‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£','bank name','bank'])
    mapped['to_bank'] = find_col(cols, ['destination bank','to bank','dest bank','‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á','destination'])
    mapped['from_account_raw'] = find_col(cols, ['‡πÄ‡∏•‡∏Ç‡∏ö‡∏±‡∏ç‡∏ä‡∏µ','account number','from account','souce id','source id','from_account','account_no'])
    mapped['to_account_raw'] = find_col(cols, ['to account','destination id','destination id number','to_account'])
    if not mapped['from_account_raw']:
        mapped['from_account_raw'] = find_col(cols, ['truemoney','‡πÄ‡∏ö‡∏≠‡∏£‡πå','‡πÄ‡∏•‡∏Ç‡∏ö‡∏±‡∏ç‡∏ä‡∏µ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ö‡∏≠‡∏£‡πå'])
    mapped['account_name'] = find_col(cols, ['‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ç‡∏ä‡∏µ','name','‡∏ä‡∏∑‡πà‡∏≠ - ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•','account name'])
    mapped['shop_name'] = find_col(cols, ['‡∏£‡πâ‡∏≤‡∏ô','‡πÄ‡∏û‡∏à','shop','website','‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå'])
    mapped['email'] = find_col(cols, ['‡∏≠‡∏µ‡πÄ‡∏°‡∏•','email','‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏µ‡πÄ‡∏°‡∏•'])
    mapped['other_accounts'] = find_col(cols, ['‡πÄ‡∏•‡∏Ç‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏≠‡∏∑‡πà‡∏ô','other account','other_accounts','other accounts'])
    mapped['notes'] = find_col(cols, ['remark','note','‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô','‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î','remarks','comment'])
    if 'Account Number' in cols and not mapped['from_account_raw']:
        mapped['from_account_raw'] = 'Account Number'
    if 'BANK NAME' in cols and not mapped['from_bank']:
        mapped['from_bank'] = 'BANK NAME'

    std = pd.DataFrame()
    std['source_file'] = os.path.basename(filename)
    std['date_str'] = df[mapped['date_str']].astype(str) if mapped['date_str'] in df.columns else ''
    std['time_str'] = df[mapped['time_str']].astype(str) if mapped['time_str'] in df.columns else ''
    def parse_dt(rdate, rtime):
        s = ''
        if pd.notna(rdate) and str(rdate).strip()!='':
            s = str(rdate).strip()
            if pd.notna(rtime) and str(rtime).strip()!='':
                s = s + ' ' + str(rtime).strip()
        return pd.to_datetime(s, dayfirst=True, errors='coerce')
    std['datetime'] = [parse_dt(d,t) for d,t in zip(std['date_str'], std['time_str'])]
    if mapped['amount'] in df.columns:
        std['amount'] = (df[mapped['amount']].astype(str).fillna('')
                        .str.replace(r'[^\d\.\-]', '', regex=True).replace('', '0').astype(float))
    else:
        std['amount'] = 0.0
    def norm_bank(x):
        if pd.isna(x): return None
        s = str(x).strip().upper()
        return None if s=='' or s.lower()=='nan' else s
    std['from_bank'] = df[mapped['from_bank']].map(norm_bank) if mapped['from_bank'] in df.columns else None
    std['to_bank']   = df[mapped['to_bank']].map(norm_bank) if mapped['to_bank'] in df.columns else None
    def get_raw(col):
        if not col or col not in df.columns:
            return pd.Series([None]*len(df))
        return df[col].astype(str).replace({'nan': None, 'None': None})
    std['from_account_raw'] = get_raw(mapped['from_account_raw'])
    std['to_account_raw']   = get_raw(mapped['to_account_raw'])
    def digits_only(s):
        if pd.isna(s): return ''
        return re.sub(r'\D+', '', str(s))
    std['from_account_digits'] = std['from_account_raw'].apply(digits_only)
    std['to_account_digits']   = std['to_account_raw'].apply(digits_only)
    std['account_name'] = get_raw(mapped['account_name']).replace({'None':None})
    std['shop_name']    = get_raw(mapped['shop_name']).replace({'None':None})
    std['email']        = get_raw(mapped['email']).replace({'None':None})
    std['other_accounts'] = get_raw(mapped['other_accounts']).replace({'None':None})
    std['notes'] = get_raw(mapped['notes']).replace({'None':None})
    std['orig_index'] = df.index
    return std.reset_index(drop=True), mapped

std_old, map_old = standardize(df_old, old_path)
std_new, map_new = standardize(df_new, new_path)

# save mapping for audit
with open("mapping_decisions.json","w",encoding="utf-8") as f:
    json.dump({'old':map_old,'new':map_new}, f, ensure_ascii=False, indent=2)

combined = pd.concat([std_old, std_new], ignore_index=True, sort=False)
combined = combined.replace({'': None})
keep_mask = ~((combined['from_account_digits'].isna() | (combined['from_account_digits']=='')) & 
              (combined['to_account_digits'].isna() | (combined['to_account_digits']=='')) & 
              (combined['amount'].fillna(0)==0) & (combined['datetime'].isna()))
combined = combined[keep_mask].copy()
combined['dup_key'] = (combined['from_account_digits'].fillna('') + '|' +
                       combined['to_account_digits'].fillna('') + '|' +
                       combined['amount'].astype(str) + '|' +
                       combined['datetime'].astype(str))
combined = combined.drop_duplicates(subset=['dup_key']).drop(columns=['dup_key'])
combined.to_csv("cleaned_combined.csv", index=False, encoding='utf-8-sig')
print("Saved cleaned_combined.csv rows:", len(combined))
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå clean & unify ‡∏ó‡∏µ‡πà‡∏ú‡∏°‡∏£‡∏±‡∏ô‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ ‚Äî ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `cleaned_combined.csv` ‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏°‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡πá‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ô‡∏µ‡πâ (‡πÅ‡∏ï‡πà‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π/‡∏õ‡∏£‡∏±‡∏ö mapping ‡πÄ‡∏≠‡∏á)

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 3 ‚Äî ‡πÇ‡∏´‡∏•‡∏î `cleaned_combined.csv` (‡∏à‡∏≤‡∏Å‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡∏ú‡∏°)

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 3 ‚Äî ‡πÇ‡∏´‡∏•‡∏î cleaned data (‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î/‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå)
import pandas as pd
df = pd.read_csv("cleaned_combined.csv", encoding='utf-8-sig')
df.shape, df.head(5)
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á cleaning ‚Äî ‡∏ñ‡πâ‡∏≤ datetime ‡πÄ‡∏õ‡πá‡∏ô NaT ‡∏´‡∏£‡∏∑‡∏≠ account\_digits ‡∏ß‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ñ‡∏ß‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á (‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå `orig_index` ‡∏ä‡πà‡∏ß‡∏¢ trace ‡∏Å‡∏•‡∏±‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á)

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 4 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á entities (nodes) ‡πÅ‡∏•‡∏∞ edges (account‚Üîname‚Üîshop‚Üîbank) ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å `nodes.csv` / `edges.csv`

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 4 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á nodes / edges
import pandas as pd, json, re
df = pd.read_csv("cleaned_combined.csv", encoding='utf-8-sig')

# map unique entities
account_ids = df['from_account_digits'].dropna().astype(str).unique().tolist()
# sometimes to_account_digits also contains accounts - add them
to_ids = df['to_account_digits'].dropna().astype(str).unique().tolist()
for t in to_ids:
    if t not in account_ids and t!='':
        account_ids.append(t)

account_map = {a: f"ACC_{i}" for i,a in enumerate(account_ids)}
name_map = {n: f"NAME_{i}" for i,n in enumerate(df['account_name'].dropna().unique().tolist())}
shop_map = {s: f"SHOP_{i}" for i,s in enumerate(df['shop_name'].dropna().unique().tolist())}
email_map = {e: f"EMAIL_{i}" for i,e in enumerate(df['email'].dropna().unique().tolist())}
bank_map = {b: f"BANK_{i}" for i,b in enumerate(df['from_bank'].dropna().unique().tolist()+df['to_bank'].dropna().unique().tolist())}

nodes = []
for raw, nid in account_map.items():
    nodes.append({'node_id':nid, 'type':'account', 'raw':raw})
for raw, nid in name_map.items():
    nodes.append({'node_id':nid, 'type':'name', 'raw':raw})
for raw, nid in shop_map.items():
    nodes.append({'node_id':nid, 'type':'shop', 'raw':raw})
for raw, nid in email_map.items():
    nodes.append({'node_id':nid, 'type':'email', 'raw':raw})
for raw, nid in bank_map.items():
    nodes.append({'node_id':nid, 'type':'bank', 'raw':raw})

edges = []
for _,r in df.iterrows():
    acc = account_map.get(str(r['from_account_digits']), None)
    if acc is None or acc=='':
        continue
    if pd.notna(r.get('account_name')):
        nm = name_map.get(r['account_name'])
        if nm: edges.append((acc, nm, 'owns'))
    if pd.notna(r.get('shop_name')):
        sh = shop_map.get(r['shop_name'])
        if sh: edges.append((acc, sh, 'used_in'))
    if pd.notna(r.get('email')):
        em = email_map.get(r['email'])
        if em: edges.append((acc, em, 'email'))
    if pd.notna(r.get('from_bank')):
        bk = bank_map.get(r['from_bank'])
        if bk: edges.append((acc, bk, 'bank'))
    # to account relation
    ta = account_map.get(str(r.get('to_account_digits', '')))
    if ta:
        edges.append((acc, ta, 'to_transfer'))

# save nodes and edges
pd.DataFrame(nodes).to_csv("nodes.csv", index=False, encoding='utf-8-sig')
pd.DataFrame(edges, columns=['source','target','relation']).to_csv("edges.csv", index=False, encoding='utf-8-sig')
print("Saved nodes.csv, edges.csv (nodes:", len(nodes), "edges:", len(edges),")")
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏´‡∏ô‡∏î‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠/‡πÄ‡∏û‡∏à/‡∏≠‡∏µ‡πÄ‡∏°‡∏•/‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£/‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ `nodes.csv` ‡πÅ‡∏•‡∏∞ `edges.csv` ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 5 ‚Äî ‡∏ó‡∏≥ fuzzy matching (optional) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° edge ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠/‡πÄ‡∏û‡∏à‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 5 ‚Äî fuzzy match ‡∏ä‡∏∑‡πà‡∏≠/‡πÄ‡∏û‡∏à (install rapidfuzz ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ)
!pip install rapidfuzz -q

from rapidfuzz import process, fuzz
import pandas as pd
nodes = pd.read_csv("nodes.csv", encoding='utf-8-sig')
edges = pd.read_csv("edges.csv", encoding='utf-8-sig')

# names
name_nodes = nodes[nodes['type']=='name']['raw'].dropna().unique().tolist()
for n in name_nodes:
    matches = process.extract(n, name_nodes, scorer=fuzz.token_sort_ratio, limit=10)
    for m,score,_ in matches:
        if m!=n and score>=85:
            src = nodes[(nodes['raw']==n)&(nodes['type']=='name')]['node_id'].values[0]
            tgt = nodes[(nodes['raw']==m)&(nodes['type']=='name')]['node_id'].values[0]
            edges = edges.append({'source':src,'target':tgt,'relation':'name_sim'}, ignore_index=True)

# shops
shop_nodes = nodes[nodes['type']=='shop']['raw'].dropna().unique().tolist()
for s in shop_nodes:
    matches = process.extract(s, shop_nodes, scorer=fuzz.token_sort_ratio, limit=10)
    for m,score,_ in matches:
        if m!=s and score>=85:
            src = nodes[(nodes['raw']==s)&(nodes['type']=='shop')]['node_id'].values[0]
            tgt = nodes[(nodes['raw']==m)&(nodes['type']=='shop')]['node_id'].values[0]
            edges = edges.append({'source':src,'target':tgt,'relation':'shop_sim'}, ignore_index=True)

edges = edges.drop_duplicates().reset_index(drop=True)
edges.to_csv("edges_with_fuzzy.csv", index=False, encoding='utf-8-sig')
print("Saved edges_with_fuzzy.csv (rows):", len(edges))
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° edge ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠/‡πÄ‡∏û‡∏à‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô (‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏™‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ú‡∏¥‡∏î/variation) ‚Äî threshold ‡πÄ‡∏õ‡πá‡∏ô 85% ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 6 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü (networkx) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì feature ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‚Üí save `node_features_basic.csv`

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 6 ‚Äî build graph + compute network features
import networkx as nx, pandas as pd
nodes = pd.read_csv("nodes.csv", encoding='utf-8-sig')
edges = pd.read_csv("edges_with_fuzzy.csv", encoding='utf-8-sig')

G = nx.Graph()
for _, r in nodes.iterrows():
    G.add_node(r['node_id'], typ=r['type'], raw=r['raw'])
for _, r in edges.iterrows():
    s,t = r['source'], r['target']
    rel = r.get('relation','')
    G.add_edge(s,t, relation=rel)

pr = nx.pagerank(G, alpha=0.85)
rows = []
for _, node in nodes.iterrows():
    nid = node['node_id']
    ntype = node['type']
    deg = G.degree(nid) if nid in G else 0
    clustering = nx.clustering(G, nid) if nid in G else 0
    pager = pr.get(nid,0)
    neigh = {'name':0,'shop':0,'email':0,'bank':0,'account':0,'other':0}
    if nid in G:
        for nb in G.neighbors(nid):
            trow = nodes[nodes['node_id']==nb]
            ttype = trow.iloc[0]['type'] if len(trow)>0 else 'other'
            if ttype not in neigh: neigh['other'] += 1
            else: neigh[ttype] += 1
    rows.append({
        'node': nid,
        'type': ntype,
        'degree': deg,
        'pagerank': pager,
        'clustering': clustering,
        'num_name_neighbors': neigh['name'],
        'num_shop_neighbors': neigh['shop'],
        'num_email_neighbors': neigh['email'],
        'num_bank_neighbors': neigh['bank'],
        'num_account_neighbors': neigh['account']
    })
pd.DataFrame(rows).to_csv("node_features_basic.csv", index=False, encoding='utf-8-sig')
print("Saved node_features_basic.csv")
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Å‡∏£‡∏≤‡∏ü‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (degree, pagerank, clustering, neighbor counts) ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô input ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ GNN/LightGBM

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 7 ‚Äî ‡∏£‡∏ß‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (assemble X.npy + node\_index\_map.json)

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 7 ‚Äî assemble numeric matrix X.npy ‡πÅ‡∏•‡∏∞ node_index_map.json
import pandas as pd, numpy as np, json

nf = pd.read_csv("node_features_basic.csv", encoding='utf-8-sig')
nodes_df = pd.read_csv("nodes.csv", encoding='utf-8-sig')

# account-level aggregated stats from cleaned_combined.csv
df = pd.read_csv("cleaned_combined.csv", encoding='utf-8-sig')
acc_stats = df.groupby('from_account_digits')['amount'].agg(['count','sum','mean']).reset_index().rename(columns={'count':'report_count','sum':'amount_sum','mean':'amount_mean'})
# map to nodes
acc_stats['node'] = acc_stats['from_account_digits'].map(lambda x: f"ACC_{list(account_map.keys()).index(x)}" if x in account_map else None) if 'account_map' in globals() else None

# safer: match nf.node to raw values in nodes.csv
# create bank dummies
bank_df = df[['from_account_digits','from_bank']].drop_duplicates()
bank_df['node'] = bank_df['from_account_digits'].map(lambda x: account_map.get(str(x)) if 'account_map' in globals() else None)

nf = nf.merge(bank_df[['node','from_bank']].rename(columns={'node':'node','from_bank':'bank_name'}), on='node', how='left')
bank_dummies = pd.get_dummies(nf['bank_name'].fillna('UNK'), prefix='BANK')
nf = pd.concat([nf.drop(columns=['bank_name']), bank_dummies], axis=1).fillna(0)

# merge acc_stats by node:
# create mapping node->raw from nodes.csv
node_to_raw = {r['node_id']: r['raw'] for _,r in nodes_df.iterrows()}
# compute acc stats mapped to nf rows
report_count = []
amount_sum = []
amount_mean = []
for _, r in nf.iterrows():
    raw = node_to_raw.get(r['node'],'')
    rows = acc_stats[acc_stats['from_account_digits']==raw] if 'acc_stats' in globals() and acc_stats is not None else pd.DataFrame()
    if len(rows)>0:
        report_count.append(rows.iloc[0].get('report_count',0))
        amount_sum.append(rows.iloc[0].get('amount_sum',0))
        amount_mean.append(rows.iloc[0].get('amount_mean',0))
    else:
        report_count.append(0)
        amount_sum.append(0)
        amount_mean.append(0)
nf['report_count'] = report_count
nf['amount_sum'] = amount_sum
nf['amount_mean'] = amount_mean

# choose feature cols
feat_cols = [c for c in nf.columns if c not in ['node','type']]
X = nf[feat_cols].values
# node_index map
node_index = {row['node']: idx for idx,row in nf.reset_index().to_dict(orient='records')}
# Better create mapping by iterating rows:
node_index = {}
for idx, row in nf.reset_index().iterrows():
    node_index[row['node']] = idx

np.save("X.npy", X)
with open("node_index_map.json","w",encoding='utf-8') as f:
    json.dump(node_index, f, ensure_ascii=False, indent=2)
pd.DataFrame({'feature':feat_cols}).to_csv("feature_cols.csv", index=False)
print("Saved X.npy (shape {}) and node_index_map.json".format(X.shape))
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏™‡∏£‡πâ‡∏≤‡∏á X matrix ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GNN input (node attributes) ‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡∏õ node->index ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô PyG / LightGBM ‡∏ï‡πà‡∏≠‡πÑ‡∏õ

> **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:** ‡∏ú‡∏°‡πÉ‡∏™‡πà‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ match `node` ‚Üí raw account ‡∏ú‡πà‡∏≤‡∏ô `nodes.csv` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏≤‡∏¢‡∏ö‡∏±‡∏ç‡∏ä‡∏µ ‡∏´‡∏≤‡∏Å mapping ‡∏ú‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à `nodes.csv` ‡πÅ‡∏•‡∏∞ `node_features_basic.csv` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 8 ‚Äî ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° label (‡∏™‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: ‡∏°‡∏µ negative / ‡πÑ‡∏°‡πà‡∏°‡∏µ negative)

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 8 ‚Äî ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° labels (y.npy)
import numpy as np, pandas as pd, json

nf = pd.read_csv("node_features_basic.csv", encoding='utf-8-sig')
nodes_df = pd.read_csv("nodes.csv", encoding='utf-8-sig')
with open("node_index_map.json","r",encoding='utf-8') as f:
    node_index = json.load(f)

df = pd.read_csv("cleaned_combined.csv", encoding='utf-8-sig')
# positives: accounts that appear as from_account_digits in cleaned_combined (reported)
black_accounts = set(df['from_account_digits'].dropna().astype(str).unique())

N = len(node_index)
y = np.zeros(N, dtype=int)

# assign positive label if node is account and raw in black_accounts
for node, idx in node_index.items():
    # only account nodes
    if node.startswith('ACC_'):
        raw = nodes_df[nodes_df['node_id']==node]['raw'].values
        if len(raw)>0 and str(raw[0]) in black_accounts:
            y[idx] = 1
        else:
            y[idx] = 0

# OPTION A: If you have an external negative list file (normal_accounts.csv with column 'account'), load it and overwrite some y[idx]=0 accordingly
# normal_df = pd.read_csv("normal_accounts.csv")
# (map to digits and set corresponding node indices to 0)

# OPTION B: If no external negatives, you can treat nodes with report_count==0 (no direct reports) as negatives (weak labeling)
# or prefer using unsupervised approach (IsolationForest) ‚Äî see alternative below.

np.save("y.npy", y)
print("Saved y.npy  positives:", int(y.sum()), "total nodes:", N)
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ `y` ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô `1` ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå negative ‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏ä‡πà‡∏ô `normal_accounts.csv`) ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ negative ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ semi-supervised / unsupervised (IsolationForest / community detection) ‡πÅ‡∏ó‡∏ô (‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 9 ‚Äî (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å) ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å/‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyG: ‡∏ó‡∏≥ PCA ‡πÄ‡∏õ‡πá‡∏ô embedding ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (fallback)

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 9a ‚Äî fallback: PCA embedding (‡∏ñ‡πâ‡∏≤ PyG ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏¢‡∏≤‡∏Å)
from sklearn.decomposition import PCA
import numpy as np, pandas as pd
X = np.load("X.npy")
pca = PCA(n_components=32, random_state=42)
emb = pca.fit_transform(X)
np.save("node_embeddings.npy", emb)
print("Saved node_embeddings.npy (PCA) shape:", emb.shape)
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏ñ‡πâ‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyG ‡∏¢‡∏≤‡∏Å ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ PCA ‡πÅ‡∏ó‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏Ç‡∏ô‡∏≤‡∏î 32 ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ train LightGBM ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prototyping)

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 10 ‚Äî ‡∏ñ‡πâ‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyG: ‡∏™‡∏£‡πâ‡∏≤‡∏á PyG Data ‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô GraphSAGE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤ node embeddings (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ neighbor sampling)

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 9b ‚Äî ‡∏ñ‡πâ‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyG: Train GraphSAGE (simple full-graph example)
import torch, numpy as np, pandas as pd, json
from torch_geometric.data import Data
from torch_geometric.nn import SAGEConv
from sklearn.model_selection import train_test_split

X = np.load("X.npy")
with open("node_index_map.json","r",encoding='utf-8') as f:
    node_index = json.load(f)
edges_df = pd.read_csv("edges_with_fuzzy.csv", encoding='utf-8-sig')

edge_list = []
for _, r in edges_df.iterrows():
    s, t = r['source'], r['target']
    if s in node_index and t in node_index:
        edge_list.append([node_index[s], node_index[t]])
        edge_list.append([node_index[t], node_index[s]])
if len(edge_list)==0:
    raise ValueError("edge_list empty ‚Äî check edges_with_fuzzy.csv and node_index_map.json")

edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
data = Data(x=torch.tensor(X, dtype=torch.float), edge_index=edge_index)

y = np.load("y.npy")
# select labeled indices where appropriate (we labeled account nodes above)
labeled_idx = np.where((y==1) | (y==0))[0]
train_idx, val_idx = train_test_split(labeled_idx, test_size=0.2, stratify=y[labeled_idx], random_state=42)
train_mask = torch.zeros(data.num_nodes, dtype=torch.bool); train_mask[train_idx]=True
val_mask = torch.zeros(data.num_nodes, dtype=torch.bool); val_mask[val_idx]=True

y_t = torch.tensor(y, dtype=torch.float)

class SAGEModel(torch.nn.Module):
    def __init__(self, in_dim, hid=64, out=32):
        super().__init__()
        self.conv1 = SAGEConv(in_dim, hid)
        self.conv2 = SAGEConv(hid, out)
        self.classifier = torch.nn.Linear(out, 1)
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SAGEModel(X.shape[1], hid=64, out=32).to(device)
opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
data = data.to(device); y_t = y_t.to(device)

for epoch in range(1,201):
    model.train()
    opt.zero_grad()
    embeddings = model(data.x, data.edge_index)
    logits = torch.sigmoid(model.classifier(embeddings).squeeze())
    loss = torch.nn.BCELoss()(logits[train_mask], y_t[train_mask])
    loss.backward()
    opt.step()
    if epoch % 20 == 0:
        model.eval()
        with torch.no_grad():
            val_score = ((logits[val_mask].cpu().numpy()>0.5).astype(int)==y[val_mask]).mean()
        print(f"epoch {epoch} loss {loss.item():.4f} val_acc approx {val_score:.4f}")

# get embeddings
model.eval()
with torch.no_grad():
    emb = model(data.x, data.edge_index).cpu().numpy()
np.save("node_embeddings.npy", emb)
print("Saved node_embeddings.npy (GNN) shape:", emb.shape)
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô‡∏ö‡∏ô GPU Colab Pro ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyG ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ô‡∏µ‡πâ ‚Äî ‡∏à‡∏∞‡πÑ‡∏î‡πâ `node_embeddings.npy` ‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÅ‡∏ö‡∏ö supervised (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ labels) ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô representation ‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥ unsupervised variant

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 11 ‚Äî ‡∏ú‡∏™‡∏≤‡∏ô embedding + tabular features ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á `train_table.csv`

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 10 ‚Äî combine embeddings with node features -> train_table.csv
import numpy as np, pandas as pd, json
emb = np.load("node_embeddings.npy")
nf = pd.read_csv("node_features_basic.csv", encoding='utf-8-sig')
with open("node_index_map.json","r",encoding='utf-8') as f:
    node_index = json.load(f)

# ensure order index mapping
rev = {v:k for k,v in node_index.items()}
rows = []
for idx in range(len(rev)):
    node = rev[idx]
    r = nf[nf['node']==node]
    base = r.iloc[0].to_dict() if len(r)>0 else {'degree':0,'pagerank':0,'clustering':0,'num_name_neighbors':0,'num_shop_neighbors':0,'num_email_neighbors':0,'num_bank_neighbors':0,'num_account_neighbors':0}
    emb_vec = emb[idx].tolist()
    for i,v in enumerate(emb_vec):
        base[f'emb_{i}'] = v
    rows.append(base)
train_df = pd.DataFrame(rows).fillna(0)
train_df['label'] = np.load("y.npy")
train_df.to_csv("train_table.csv", index=False, encoding='utf-8-sig')
print("Saved train_table.csv shape:", train_df.shape)
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Å‡∏£‡∏≤‡∏ü‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô + embedding ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå `label` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å LightGBM

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 12 ‚Äî ‡πÄ‡∏ó‡∏£‡∏ô LightGBM (final classifier) ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 11 ‚Äî Train LightGBM on train_table.csv
import lightgbm as lgb, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report, precision_recall_curve

df = pd.read_csv("train_table.csv", encoding='utf-8-sig')
X = df.drop(columns=['label']).values
y = df['label'].values

# simple split
X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

dtrain = lgb.Dataset(X_train, label=y_train)
dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)
params = {
    'objective':'binary',
    'metric':'auc',
    'boosting_type':'gbdt',
    'verbosity': -1,
    'is_unbalance': True,
    'learning_rate':0.05,
    'num_leaves': 63,
    'seed': 42
}
bst = lgb.train(params, dtrain, valid_sets=[dval], num_boost_round=1000, early_stopping_rounds=50)
bst.save_model('lightgbm_model.txt')
y_prob = bst.predict(X_val)
print("AUC:", roc_auc_score(y_val, y_prob))

# choose threshold by F1
prec, rec, thr = precision_recall_curve(y_val, y_prob)
f1_scores = 2*prec*rec/(prec+rec+1e-12)
best_idx = f1_scores.argmax()
best_thr = thr[best_idx] if best_idx < len(thr) else 0.5
y_pred = (y_prob >= best_thr).astype(int)
print("Chosen threshold:", best_thr)
print(classification_report(y_val, y_pred))
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡πÄ‡∏ó‡∏£‡∏ô LightGBM ‡∏ö‡∏ô features ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå `lightgbm_model.txt` ‚Äî ‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå `num_leaves`, `learning_rate` ‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î dataset

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 13 ‚Äî ‡πÄ‡∏≠‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡πÉ‡∏ä‡πâ (inference) / export top-K flagged accounts

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 12 ‚Äî inference example: score all nodes, export top-k
import numpy as np, pandas as pd, joblib
import lightgbm as lgb
df_train = pd.read_csv("train_table.csv", encoding='utf-8-sig')
X_all = df_train.drop(columns=['label']).values
bst = lgb.Booster(model_file='lightgbm_model.txt')
probs = bst.predict(X_all)
df_out = pd.read_csv("node_features_basic.csv", encoding='utf-8-sig')
df_out = df_out.reset_index(drop=True)
df_out['prob'] = probs
df_out.sort_values('prob', ascending=False, inplace=True)
df_out.head(50).to_csv("top50_flagged.csv", index=False, encoding='utf-8-sig')
print("Saved top50_flagged.csv")
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å node ‡πÅ‡∏•‡πâ‡∏ß export top-N ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏´‡πâ analyst ‡∏ï‡∏£‡∏ß‡∏à

---

### ‡πÄ‡∏ã‡∏•‡∏•‡πå 14 ‚Äî ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå/‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å Colab

```python
# ‡πÄ‡∏ã‡∏•‡∏•‡πå 13 ‚Äî ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
from google.colab import files
files.download('cleaned_combined.csv')       # ‡∏ñ‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Colab
files.download('mapping_decisions.json')
files.download('nodes.csv')
files.download('edges_with_fuzzy.csv')
files.download('node_features_basic.csv')
files.download('X.npy')
files.download('node_index_map.json')
files.download('node_embeddings.npy')
files.download('train_table.csv')
files.download('lightgbm_model.txt')
files.download('top50_flagged.csv')
```

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á/Drive

---

## ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô & ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Threshold (‡∏™‡∏£‡∏∏‡∏õ)

* **Metric:** PR-AUC, ROC-AUC, Precision\@k, Recall\@k ‚Äî ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• imbalance ‡πÉ‡∏´‡πâ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö **Precision\@k** (top flagged ‡∏ó‡∏µ‡πà analyst ‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á)
* **Threshold:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ business trade-off ‚Äî ‡∏ñ‡πâ‡∏≤ false positive ‡πÅ‡∏û‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô (precision‚Üë)
* **‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÇ‡∏î‡∏¢‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå:** ‡∏™‡πà‡∏á Top-K ‡πÉ‡∏´‡πâ analyst ‡∏ï‡∏£‡∏ß‡∏à ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö feedback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÑ‡∏õ active learning (retrain)

---

## Deployment (‡∏™‡∏±‡πâ‡∏ô ‡πÜ)

* **Batch:** ‡∏£‡∏µ‡∏£‡∏±‡∏ô pipeline ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏∑‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå ‚Üí ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï graph/embeddings ‚Üí retrain/refresh model
* **Near-real-time:** ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ report ‡πÉ‡∏´‡∏°‡πà ‚Üí update graph ‚Üí compute incremental features (degree neighbors) ‚Üí compute inductive embedding (GraphSAGE) ‡∏´‡∏£‡∏∑‡∏≠ use stored embedding + neighbor features ‚Üí predict with LightGBM ‚Üí if prob>threshold push to analyst queue
* **Stack:** Kafka ‚Üí Feature store (Feast) ‚Üí Model serving (FastAPI / TorchServe) ‚Üí UI for analyst

---

## Monitoring & Continuous Learning

* ‡πÄ‡∏Å‡πá‡∏ö log ‡∏ó‡∏∏‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à: account\_id, prob, decision, analyst\_feedback, timestamp
* KPI: Precision\@100, Alerts/day/analyst, False Positive Rate, Time-to-review
* Scheduled retrain (weekly/monthly) + active learning (‡πÄ‡∏û‡∏¥‡πà‡∏° labeled samples ‡∏à‡∏≤‡∏Å analyst)

---

## ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)

1. **Privacy (PDPA):** ‡∏≠‡∏¢‡πà‡∏≤‡πÅ‡∏ä‡∏£‡πå PII ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï ‚Äî ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ä‡∏£‡πå‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡∏Å‡∏ô‡∏≠‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡πÉ‡∏´‡πâ mask/hash ‡πÄ‡∏•‡∏Ç‡∏ö‡∏±‡∏ç‡∏ä‡∏µ/‡πÄ‡∏ö‡∏≠‡∏£‡πå/‡∏ä‡∏∑‡πà‡∏≠
2. **Label bias:** blacklist ‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô positives ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ‚Äî ‡∏´‡∏≤‡∏°‡∏≤ negative ‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ semi-supervised ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á bias
3. **Scalability:** ‡∏Å‡∏£‡∏≤‡∏ü‡πÉ‡∏´‡∏ç‡πà‡∏ï‡πâ‡∏≠‡∏á batch/neighbor sampling ‡πÅ‡∏•‡∏∞ graph DB (Neo4j) ‡∏´‡∏£‡∏∑‡∏≠ PyG mini-batch

---

## ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ú‡∏°‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏´‡πâ (‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î)

* [‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î `cleaned_combined.csv`](sandbox:/mnt/data/cleaned_combined.csv)
* [‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î `mapping_decisions.json`](sandbox:/mnt/data/mapping_decisions.json)

(‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏ú‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ zip ‡∏ó‡∏∏‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏¢‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ ‚Äî ‡∏ö‡∏≠‡∏Å‡∏ú‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)

---

‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏ú‡∏°‡∏à‡∏∞:

* ‡∏™‡∏£‡πâ‡∏≤‡∏á **Colab .ipynb** ‡∏û‡∏£‡πâ‡∏≠‡∏° cell ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î (‡∏ú‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå notebook ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î)
* ‡∏´‡∏£‡∏∑‡∏≠ **‡∏£‡∏±‡∏ô pipeline** ‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ö‡∏ô environment ‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏£‡∏ö‡∏ä‡∏∏‡∏î‡πÉ‡∏´‡πâ (nodes/edges/emb/X.npy/embeddings/lightgbm model/topk) ‚Äî ‡∏ú‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏î‡πâ‡∏ß‡∏¢ (‡∏ö‡∏≠‡∏Å‡∏ú‡∏°‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏°‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô‡∏ö‡∏ô Colab)

‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡∏ú‡∏°‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô:

1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `colab_pipeline.ipynb` ‡πÉ‡∏´‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏•‡∏¢?
2. ‡∏ú‡∏°‡∏£‡∏±‡∏ô pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏£‡∏ö‡∏ä‡∏∏‡∏î (nodes/edges/X/emb/model/top50)?
3. ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏°‡∏ó‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô GNN (‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ GPU) ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á model?

‡∏ö‡∏≠‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‚Äî ‡∏ú‡∏°‡∏à‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≠‡∏ó‡∏±‡∏ô‡∏ó‡∏µ üòä
